{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This is an executable notebook version of [Building powerful image classification models using very little data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from easyfinetune.datasets import dogs_vs_cats\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import Model\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: /tmp/dogs_vs_cats.zip already exists, skipping download.  Delete file to override this\n",
      "WARNING: /tmp/dogscats already exists, skipping extract.  Delete dir to override this\n"
     ]
    }
   ],
   "source": [
    "dogs_vs_cats_data = dogs_vs_cats.download(extract=True)\n",
    "train_data_dir = os.path.join(dogs_vs_cats_data, \"train\")\n",
    "validation_data_dir = os.path.join(dogs_vs_cats_data, \"valid\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train fully connected layer (aka \"bottleneck features\") to 90% accuracy\n",
    "\n",
    "### Overview\n",
    "\n",
    "Using the bottleneck features of a pre-trained network: 90% accuracy in a minute.\n",
    "A more refined approach would be to leverage a network pre-trained on a large dataset. Such a network would have already learned features that are useful for most computer vision problems, and leveraging such features would allow us to reach a better accuracy than any method that would only rely on the available data.\n",
    "\n",
    "We will use the VGG16 architecture, pre-trained on the ImageNet dataset. Because the ImageNet dataset contains several \"cat\" classes (persian cat, siamese cat...) and many \"dog\" classes among its total of 1000 classes, this model will already have learned features that are relevant to our classification problem. In fact, it is possible that merely recording the softmax predictions of the model over our data rather than the bottleneck features would be enough to solve our dogs vs. cats classification problem extremely well. However, the method we present here is more likely to generalize well to a broader range of problems, including problems featuring classes absent from ImageNet.\n",
    "\n",
    "Here's what the VGG16 architecture looks like:\n",
    "\n",
    "![vgg16_original.png](images/vgg16_original.png)\n",
    "\n",
    "Our strategy will be as follows: we will only instantiate the convolutional part of the model, everything up to the fully-connected layers. We will then run this model on our training and validation data once, recording the output (the \"bottleneck features\" from th VGG16 model: the last activation maps before the fully-connected layers) in two numpy arrays. Then we will train a small fully-connected model on top of the stored features.\n",
    "\n",
    "The motivation storing the features offline rather than adding our fully-connected model directly on top of a frozen convolutional base and running the whole thing is due to computational effiency.  Running VGG16 is expensive, especially if you're working on CPU, and we want to only do it once. Note that this prevents us from using data augmentation in this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "epochs = 25\n",
    "\n",
    "img_width, img_height = dogs_vs_cats.img_width, dogs_vs_cats.img_height\n",
    "num_train_samples = dogs_vs_cats.num_train_samples\n",
    "num_validation_samples = dogs_vs_cats.num_validation_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Prepare pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get the pretrained VGG16 network _without_ the top layer (fully connected layer), since we need to train that\n",
    "# on two categories (dogs vs cats) rather than the full set of Imagenet categories that comes with it.\n",
    "model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate bottleneck features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that does a *single* forward pass through the convolutional blocks of the network with the full dataset and save the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bottlebeck_features(model):\n",
    "    \n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    \n",
    "    bottleneck_features_train = model.predict_generator(\n",
    "        generator, \n",
    "        num_train_samples // batch_size, \n",
    "        verbose=1,\n",
    "    )\n",
    "    \n",
    "    generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    \n",
    "    bottleneck_features_validation = model.predict_generator(\n",
    "        generator, \n",
    "        num_validation_samples // batch_size, \n",
    "        verbose=1,\n",
    "    )\n",
    "        \n",
    "    return (bottleneck_features_train, bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this model on our training and validation data once, recording the output \n",
    "(the \"bottleneck features\" from th VGG16 model: the last activation maps before the fully-connected layers) \n",
    "in two numpy arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "2300/2300 [==============================] - 112s 49ms/step\n",
      "Found 2000 images belonging to 2 classes.\n",
      "200/200 [==============================] - 12s 58ms/step\n"
     ]
    }
   ],
   "source": [
    "bottleneck_features_train, bottleneck_features_validation = generate_bottlebeck_features(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train fully connected layer (aka \"Top-Model\")\n",
    "\n",
    "Train a small fully-connected model on top of the bottleneck features generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_top_model(bottleneck_features_train, bottleneck_features_validation):\n",
    "    \n",
    "    train_data = bottleneck_features_train\n",
    "    train_labels = np.array([0] * (num_train_samples // 2) + [1] * (num_train_samples // 2))\n",
    "\n",
    "    validation_data = bottleneck_features_validation\n",
    "    validation_labels = np.array([0] * (num_validation_samples // 2) + [1] * (num_validation_samples // 2))\n",
    "\n",
    "    top_model = Sequential()\n",
    "    top_model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    top_model.add(Dense(256, activation='relu'))\n",
    "    top_model.add(Dropout(0.5))\n",
    "    top_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    top_model.compile(\n",
    "        optimizer='rmsprop',          \n",
    "        loss='binary_crossentropy', \n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "\n",
    "    top_model.fit(train_data, \n",
    "              train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(validation_data, validation_labels))\n",
    "    \n",
    "    return top_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/25\n",
      "23000/23000 [==============================] - 11s 499us/step - loss: 0.3792 - acc: 0.8556 - val_loss: 0.4995 - val_acc: 0.8195\n",
      "Epoch 2/25\n",
      "23000/23000 [==============================] - 13s 547us/step - loss: 0.3017 - acc: 0.8880 - val_loss: 0.2440 - val_acc: 0.9135\n",
      "Epoch 3/25\n",
      "23000/23000 [==============================] - 13s 553us/step - loss: 0.2727 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.9090\n",
      "Epoch 4/25\n",
      "23000/23000 [==============================] - 11s 490us/step - loss: 0.2642 - acc: 0.9031 - val_loss: 0.2651 - val_acc: 0.9015\n",
      "Epoch 5/25\n",
      "23000/23000 [==============================] - 13s 545us/step - loss: 0.2553 - acc: 0.9071 - val_loss: 0.2580 - val_acc: 0.9105\n",
      "Epoch 6/25\n",
      "23000/23000 [==============================] - 11s 483us/step - loss: 0.2519 - acc: 0.9115 - val_loss: 0.2645 - val_acc: 0.9105\n",
      "Epoch 7/25\n",
      "23000/23000 [==============================] - 11s 482us/step - loss: 0.2524 - acc: 0.9130 - val_loss: 0.2773 - val_acc: 0.9180\n",
      "Epoch 8/25\n",
      "23000/23000 [==============================] - 12s 514us/step - loss: 0.2541 - acc: 0.9115 - val_loss: 0.2545 - val_acc: 0.9105\n",
      "Epoch 9/25\n",
      "23000/23000 [==============================] - 11s 487us/step - loss: 0.2478 - acc: 0.9155 - val_loss: 0.2906 - val_acc: 0.9155\n",
      "Epoch 10/25\n",
      "23000/23000 [==============================] - 11s 494us/step - loss: 0.2492 - acc: 0.9154 - val_loss: 0.2982 - val_acc: 0.9035\n",
      "Epoch 11/25\n",
      "23000/23000 [==============================] - 11s 475us/step - loss: 0.2472 - acc: 0.9144 - val_loss: 0.3048 - val_acc: 0.9145\n",
      "Epoch 12/25\n",
      "23000/23000 [==============================] - 14s 598us/step - loss: 0.2464 - acc: 0.9167 - val_loss: 0.2901 - val_acc: 0.9120\n",
      "Epoch 13/25\n",
      "23000/23000 [==============================] - 11s 492us/step - loss: 0.2490 - acc: 0.9187 - val_loss: 0.3221 - val_acc: 0.9070\n",
      "Epoch 14/25\n",
      "23000/23000 [==============================] - 11s 486us/step - loss: 0.2442 - acc: 0.9194 - val_loss: 0.3309 - val_acc: 0.9125\n",
      "Epoch 15/25\n",
      "23000/23000 [==============================] - 11s 470us/step - loss: 0.2462 - acc: 0.9232 - val_loss: 0.3007 - val_acc: 0.9135\n",
      "Epoch 16/25\n",
      "23000/23000 [==============================] - 12s 501us/step - loss: 0.2486 - acc: 0.9207 - val_loss: 0.3411 - val_acc: 0.9095\n",
      "Epoch 17/25\n",
      "23000/23000 [==============================] - 11s 498us/step - loss: 0.2400 - acc: 0.9220 - val_loss: 0.3422 - val_acc: 0.9160\n",
      "Epoch 18/25\n",
      "23000/23000 [==============================] - 11s 462us/step - loss: 0.2471 - acc: 0.9224 - val_loss: 0.3293 - val_acc: 0.9145\n",
      "Epoch 19/25\n",
      "23000/23000 [==============================] - 11s 477us/step - loss: 0.2385 - acc: 0.9236 - val_loss: 0.3542 - val_acc: 0.9150\n",
      "Epoch 20/25\n",
      "23000/23000 [==============================] - 11s 492us/step - loss: 0.2381 - acc: 0.9261 - val_loss: 0.3651 - val_acc: 0.9140\n",
      "Epoch 21/25\n",
      "23000/23000 [==============================] - 12s 543us/step - loss: 0.2394 - acc: 0.9267 - val_loss: 0.3548 - val_acc: 0.9225\n",
      "Epoch 22/25\n",
      "23000/23000 [==============================] - 12s 501us/step - loss: 0.2419 - acc: 0.9269 - val_loss: 0.3941 - val_acc: 0.9055\n",
      "Epoch 23/25\n",
      "23000/23000 [==============================] - 10s 449us/step - loss: 0.2343 - acc: 0.9302 - val_loss: 0.3882 - val_acc: 0.9160\n",
      "Epoch 24/25\n",
      "23000/23000 [==============================] - 11s 489us/step - loss: 0.2400 - acc: 0.9296 - val_loss: 0.3613 - val_acc: 0.9055\n",
      "Epoch 25/25\n",
      "23000/23000 [==============================] - 12s 516us/step - loss: 0.2361 - acc: 0.9304 - val_loss: 0.3898 - val_acc: 0.9200\n"
     ]
    }
   ],
   "source": [
    "top_model = train_top_model(bottleneck_features_train, bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further improve our previous result, we can try to \"fine-tune\" the last convolutional block of the VGG16 model alongside the top-level classifier. Fine-tuning consist in starting from a trained network, then re-training it on a new dataset using very small weight updates. In our case, this can be done in 3 steps:\n",
    "\n",
    "1. Instantiate the convolutional base of VGG16 and load its weights\n",
    "1. Add our previously defined fully-connected model on top, and load its weights\n",
    "1. Freeze the layers of the VGG16 model up to the last convolutional block\n",
    "\n",
    "![vgg16_modified.png](images/vgg16_modified.png)\n",
    "\n",
    "Note that:\n",
    "\n",
    "* In order to perform fine-tuning, all layers should start with properly trained weights: for instance you should not slap a randomly initialized fully-connected network on top of a pre-trained convolutional base. This is because the large gradient updates triggered by the randomly initialized weights would wreck the learned weights in the convolutional base. In our case this is why we first train the top-level classifier, and only then start fine-tuning convolutional weights alongside it.\n",
    "* We choose to only fine-tune the last convolutional block rather than the entire network in order to prevent overfitting, since the entire network would have a very large entropic capacity and thus a strong tendency to overfit. The features learned by low-level convolutional blocks are more general, less abstract than those found higher-up, so it is sensible to keep the first few blocks fixed (more general features) and only fine-tune the last one (more specialized features).\n",
    "* Fine-tuning should be done with a very slow learning rate, and typically with the SGD optimizer rather than an adaptative learning rate optimizer such as RMSProp. This is to make sure that the magnitude of the updates stays very small, so as not to wreck the previously learned features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set additional params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "momentum = 0.9\n",
    "\n",
    "# this is the number of layers that contains the first 4 (of 5 total) convblocks.\n",
    "# this is special because these are the layers that we will freeze, whereas convblock 5\n",
    "# will be fine-tuned.\n",
    "num_first_4_convblock_layers = 15 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate base model and freeze first four conv blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = applications.VGG16(\n",
    "    weights='imagenet', \n",
    "    input_shape=(img_width, img_height, 3), \n",
    "    include_top=False\n",
    ")\n",
    "\n",
    "# Freeze first four conv blocks\n",
    "for layer in base_model.layers[:num_first_4_convblock_layers]:\n",
    "    layer.trainable = False  # aka \"freeze\" this layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine base model with previously trained top model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tleyden/Development/easy-fine-tune/venv/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"se...)`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# add the model on top of the convolutional base\n",
    "model = Model(\n",
    "    input= base_model.input, \n",
    "    output=top_model(base_model.output)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=learning_rate, momentum=momentum),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect model to input generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "2300/2300 [==============================] - 207s 90ms/step - loss: 0.4167 - acc: 0.8084 - val_loss: 0.1939 - val_acc: 0.9365\n",
      "Epoch 2/25\n",
      "2300/2300 [==============================] - 203s 88ms/step - loss: 0.2103 - acc: 0.9175 - val_loss: 0.1584 - val_acc: 0.9500\n",
      "Epoch 3/25\n",
      "2300/2300 [==============================] - 196s 85ms/step - loss: 0.1682 - acc: 0.9357 - val_loss: 0.1980 - val_acc: 0.9490\n",
      "Epoch 4/25\n",
      "2300/2300 [==============================] - 203s 88ms/step - loss: 0.1408 - acc: 0.9440 - val_loss: 0.1404 - val_acc: 0.9470\n",
      "Epoch 5/25\n",
      "2300/2300 [==============================] - 210s 91ms/step - loss: 0.1246 - acc: 0.9533 - val_loss: 0.1394 - val_acc: 0.9530\n",
      "Epoch 6/25\n",
      "2300/2300 [==============================] - 200s 87ms/step - loss: 0.1133 - acc: 0.9550 - val_loss: 0.1287 - val_acc: 0.9590\n",
      "Epoch 7/25\n",
      "2300/2300 [==============================] - 195s 85ms/step - loss: 0.0947 - acc: 0.9613 - val_loss: 0.1226 - val_acc: 0.9575\n",
      "Epoch 8/25\n",
      "2300/2300 [==============================] - 199s 86ms/step - loss: 0.0930 - acc: 0.9643 - val_loss: 0.1473 - val_acc: 0.9600\n",
      "Epoch 9/25\n",
      "2300/2300 [==============================] - 199s 87ms/step - loss: 0.0829 - acc: 0.9674 - val_loss: 0.1692 - val_acc: 0.9580\n",
      "Epoch 10/25\n",
      "2300/2300 [==============================] - 191s 83ms/step - loss: 0.0754 - acc: 0.9723 - val_loss: 0.1831 - val_acc: 0.9500\n",
      "Epoch 11/25\n",
      "2300/2300 [==============================] - 189s 82ms/step - loss: 0.0682 - acc: 0.9746 - val_loss: 0.1405 - val_acc: 0.9595\n",
      "Epoch 12/25\n",
      "2300/2300 [==============================] - 195s 85ms/step - loss: 0.0650 - acc: 0.9758 - val_loss: 0.1332 - val_acc: 0.9590\n",
      "Epoch 13/25\n",
      "2300/2300 [==============================] - 203s 88ms/step - loss: 0.0584 - acc: 0.9782 - val_loss: 0.1457 - val_acc: 0.9650\n",
      "Epoch 14/25\n",
      "2300/2300 [==============================] - 188s 82ms/step - loss: 0.0541 - acc: 0.9795 - val_loss: 0.1825 - val_acc: 0.9635\n",
      "Epoch 15/25\n",
      "2300/2300 [==============================] - 202s 88ms/step - loss: 0.0583 - acc: 0.9794 - val_loss: 0.1611 - val_acc: 0.9630\n",
      "Epoch 16/25\n",
      "2300/2300 [==============================] - 206s 90ms/step - loss: 0.0436 - acc: 0.9852 - val_loss: 0.2001 - val_acc: 0.9580\n",
      "Epoch 17/25\n",
      "2300/2300 [==============================] - 187s 82ms/step - loss: 0.0471 - acc: 0.9834 - val_loss: 0.1441 - val_acc: 0.9615\n",
      "Epoch 18/25\n",
      "2300/2300 [==============================] - 196s 85ms/step - loss: 0.0428 - acc: 0.9858 - val_loss: 0.1392 - val_acc: 0.9610\n",
      "Epoch 19/25\n",
      "2300/2300 [==============================] - 198s 86ms/step - loss: 0.0374 - acc: 0.9876 - val_loss: 0.1238 - val_acc: 0.9615\n",
      "Epoch 20/25\n",
      "2300/2300 [==============================] - 199s 87ms/step - loss: 0.0376 - acc: 0.9867 - val_loss: 0.1814 - val_acc: 0.9625\n",
      "Epoch 21/25\n",
      "2300/2300 [==============================] - 188s 82ms/step - loss: 0.0396 - acc: 0.9866 - val_loss: 0.2005 - val_acc: 0.9620\n",
      "Epoch 22/25\n",
      "2300/2300 [==============================] - 197s 85ms/step - loss: 0.0341 - acc: 0.9884 - val_loss: 0.1952 - val_acc: 0.9655\n",
      "Epoch 23/25\n",
      "2300/2300 [==============================] - 179s 78ms/step - loss: 0.0348 - acc: 0.9887 - val_loss: 0.1998 - val_acc: 0.9650\n",
      "Epoch 24/25\n",
      "2300/2300 [==============================] - 195s 85ms/step - loss: 0.0296 - acc: 0.9903 - val_loss: 0.2014 - val_acc: 0.9665\n",
      "Epoch 25/25\n",
      "2300/2300 [==============================] - 203s 88ms/step - loss: 0.0288 - acc: 0.9900 - val_loss: 0.2075 - val_acc: 0.9655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4d4622d0b8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=num_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=num_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
