{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This is an executable notebook version of [Building powerful image classification models using very little data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyfinetune.datasets import dogs_vs_cats\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import Model\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dogs_vs_cats_data = dogs_vs_cats.download(extract=True)\n",
    "train_data_dir = os.path.join(dogs_vs_cats_data, \"train\")\n",
    "validation_data_dir = os.path.join(dogs_vs_cats_data, \"valid\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train fully connected layer (aka \"bottleneck features\") to 90% accuracy\n",
    "\n",
    "### Overview\n",
    "\n",
    "Using the bottleneck features of a pre-trained network: 90% accuracy in a minute\n",
    "A more refined approach would be to leverage a network pre-trained on a large dataset. Such a network would have already learned features that are useful for most computer vision problems, and leveraging such features would allow us to reach a better accuracy than any method that would only rely on the available data.\n",
    "\n",
    "We will use the VGG16 architecture, pre-trained on the ImageNet dataset --a model previously featured on this blog. Because the ImageNet dataset contains several \"cat\" classes (persian cat, siamese cat...) and many \"dog\" classes among its total of 1000 classes, this model will already have learned features that are relevant to our classification problem. In fact, it is possible that merely recording the softmax predictions of the model over our data rather than the bottleneck features would be enough to solve our dogs vs. cats classification problem extremely well. However, the method we present here is more likely to generalize well to a broader range of problems, including problems featuring classes absent from ImageNet.\n",
    "\n",
    "Here's what the VGG16 architecture looks like:\n",
    "\n",
    "![vgg16_original.png](images/vgg16_original.png)\n",
    "\n",
    "Our strategy will be as follow: we will only instantiate the convolutional part of the model, everything up to the fully-connected layers. We will then run this model on our training and validation data once, recording the output (the \"bottleneck features\" from th VGG16 model: the last activation maps before the fully-connected layers) in two numpy arrays. Then we will train a small fully-connected model on top of the stored features.\n",
    "\n",
    "The reason why we are storing the features offline rather than adding our fully-connected model directly on top of a frozen convolutional base and running the whole thing, is computational effiency. Running VGG16 is expensive, especially if you're working on CPU, and we want to only do it once. Note that this prevents us from using data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# epochs = 50\n",
    "epochs = 10\n",
    "batch_size = 10\n",
    "\n",
    "img_width, img_height = dogs_vs_cats.img_width, dogs_vs_cats.img_height\n",
    "num_train_samples = dogs_vs_cats.num_train_samples\n",
    "num_validation_samples = dogs_vs_cats.num_validation_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Prepare pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build the VGG16 network\n",
    "model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate bottleneck features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that does a *single* forward pass through the convolutional blocks of the network with the full dataset and save the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bottlebeck_features(model):\n",
    "    \n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    \n",
    "    bottleneck_features_train = model.predict_generator(\n",
    "        generator, \n",
    "        num_train_samples // batch_size, \n",
    "        verbose=1,\n",
    "    )\n",
    "    \n",
    "    generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    \n",
    "    bottleneck_features_validation = model.predict_generator(\n",
    "        generator, \n",
    "        num_validation_samples // batch_size, \n",
    "        verbose=1,\n",
    "    )\n",
    "        \n",
    "    return (bottleneck_features_train, bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this model on our training and validation data once, recording the output \n",
    "(the \"bottleneck features\" from th VGG16 model: the last activation maps before the fully-connected layers) \n",
    "in two numpy arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "2300/2300 [==============================] - 114s 49ms/step\n",
      "Found 2000 images belonging to 2 classes.\n",
      "200/200 [==============================] - 12s 62ms/step\n"
     ]
    }
   ],
   "source": [
    "bottleneck_features_train, bottleneck_features_validation = generate_bottlebeck_features(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train fully connected layer (aka \"Top-Model\")\n",
    "\n",
    "Train a small fully-connected model on top of the bottleneck features generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_top_model(bottleneck_features_train, bottleneck_features_validation):\n",
    "    \n",
    "    train_data = bottleneck_features_train\n",
    "    train_labels = np.array([0] * (num_train_samples // 2) + [1] * (num_train_samples // 2))\n",
    "\n",
    "    validation_data = bottleneck_features_validation\n",
    "    validation_labels = np.array([0] * (num_validation_samples // 2) + [1] * (num_validation_samples // 2))\n",
    "\n",
    "    top_model = Sequential()\n",
    "    top_model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    top_model.add(Dense(256, activation='relu'))\n",
    "    top_model.add(Dropout(0.5))\n",
    "    top_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    top_model.compile(\n",
    "        optimizer='rmsprop',          \n",
    "        loss='binary_crossentropy', \n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "\n",
    "    top_model.fit(train_data, \n",
    "              train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(validation_data, validation_labels))\n",
    "    \n",
    "    return top_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "23000/23000 [==============================] - 11s 499us/step - loss: 0.3804 - acc: 0.8582 - val_loss: 0.2675 - val_acc: 0.8870\n",
      "Epoch 2/10\n",
      "23000/23000 [==============================] - 11s 479us/step - loss: 0.3038 - acc: 0.8873 - val_loss: 0.2597 - val_acc: 0.9045\n",
      "Epoch 3/10\n",
      "23000/23000 [==============================] - 12s 506us/step - loss: 0.2744 - acc: 0.8999 - val_loss: 0.3595 - val_acc: 0.8970\n",
      "Epoch 4/10\n",
      "23000/23000 [==============================] - 11s 497us/step - loss: 0.2656 - acc: 0.9042 - val_loss: 0.3098 - val_acc: 0.8915\n",
      "Epoch 5/10\n",
      "23000/23000 [==============================] - 11s 497us/step - loss: 0.2582 - acc: 0.9057 - val_loss: 0.2810 - val_acc: 0.9025\n",
      "Epoch 6/10\n",
      "23000/23000 [==============================] - 12s 506us/step - loss: 0.2557 - acc: 0.9109 - val_loss: 0.3416 - val_acc: 0.8905\n",
      "Epoch 7/10\n",
      "23000/23000 [==============================] - 12s 537us/step - loss: 0.2542 - acc: 0.9115 - val_loss: 0.2798 - val_acc: 0.9040\n",
      "Epoch 8/10\n",
      "23000/23000 [==============================] - 12s 502us/step - loss: 0.2513 - acc: 0.9145 - val_loss: 0.2779 - val_acc: 0.9150\n",
      "Epoch 9/10\n",
      "23000/23000 [==============================] - 12s 501us/step - loss: 0.2470 - acc: 0.9144 - val_loss: 0.3343 - val_acc: 0.9125\n",
      "Epoch 10/10\n",
      "23000/23000 [==============================] - 12s 542us/step - loss: 0.2506 - acc: 0.9153 - val_loss: 0.2926 - val_acc: 0.9125\n"
     ]
    }
   ],
   "source": [
    "top_model = train_top_model(bottleneck_features_train, bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further improve our previous result, we can try to \"fine-tune\" the last convolutional block of the VGG16 model alongside the top-level classifier. Fine-tuning consist in starting from a trained network, then re-training it on a new dataset using very small weight updates. In our case, this can be done in 3 steps:\n",
    "\n",
    "1. Instantiate the convolutional base of VGG16 and load its weights\n",
    "1. Add our previously defined fully-connected model on top, and load its weights\n",
    "1. Freeze the layers of the VGG16 model up to the last convolutional block\n",
    "\n",
    "![vgg16_modified.png](images/vgg16_modified.png)\n",
    "\n",
    "Note that:\n",
    "\n",
    "* In order to perform fine-tuning, all layers should start with properly trained weights: for instance you should not slap a randomly initialized fully-connected network on top of a pre-trained convolutional base. This is because the large gradient updates triggered by the randomly initialized weights would wreck the learned weights in the convolutional base. In our case this is why we first train the top-level classifier, and only then start fine-tuning convolutional weights alongside it.\n",
    "* We choose to only fine-tune the last convolutional block rather than the entire network in order to prevent overfitting, since the entire network would have a very large entropic capacity and thus a strong tendency to overfit. The features learned by low-level convolutional blocks are more general, less abstract than those found higher-up, so it is sensible to keep the first few blocks fixed (more general features) and only fine-tune the last one (more specialized features).\n",
    "* Fine-tuning should be done with a very slow learning rate, and typically with the SGD optimizer rather than an adaptative learning rate optimizer such as RMSProp. This is to make sure that the magnitude of the updates stays very small, so as not to wreck the previously learned features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set additional params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "momentum = 0.9\n",
    "\n",
    "# this is the number of layers that contains the first 4 (of 5 total) convblocks.\n",
    "# this is special because these are the layers that we will freeze, whereas convblock 5\n",
    "# will be fine-tuned.\n",
    "num_first_4_convblock_layers = 15 \n",
    "\n",
    "batch_size = 64\n",
    "epochs = 25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate base model and freeze first four conv blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = applications.VGG16(\n",
    "    weights='imagenet', \n",
    "    input_shape=(img_width, img_height, 3), \n",
    "    include_top=False\n",
    ")\n",
    "\n",
    "# Freeze first four conv blocks\n",
    "for layer in base_model.layers[:num_first_4_convblock_layers]:\n",
    "    layer.trainable = False  # aka \"freeze\" this layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine base model with previously trained top model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tleyden/Development/easy-fine-tune/venv/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"se...)`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# add the model on top of the convolutional base\n",
    "model = Model(\n",
    "    input= base_model.input, \n",
    "    output=top_model(base_model.output)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=learning_rate, momentum=momentum),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect model to input generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "359/359 [==============================] - 167s 466ms/step - loss: 0.2429 - acc: 0.9061 - val_loss: 0.1905 - val_acc: 0.9350\n",
      "Epoch 2/25\n",
      "359/359 [==============================] - 154s 430ms/step - loss: 0.1974 - acc: 0.9237 - val_loss: 0.1723 - val_acc: 0.9427\n",
      "Epoch 3/25\n",
      "359/359 [==============================] - 158s 441ms/step - loss: 0.1778 - acc: 0.9301 - val_loss: 0.1533 - val_acc: 0.9458\n",
      "Epoch 4/25\n",
      "359/359 [==============================] - 156s 435ms/step - loss: 0.1616 - acc: 0.9352 - val_loss: 0.1583 - val_acc: 0.9375\n",
      "Epoch 5/25\n",
      "359/359 [==============================] - 157s 436ms/step - loss: 0.1493 - acc: 0.9406 - val_loss: 0.1541 - val_acc: 0.9447\n",
      "Epoch 6/25\n",
      "359/359 [==============================] - 152s 424ms/step - loss: 0.1436 - acc: 0.9430 - val_loss: 0.1300 - val_acc: 0.9525\n",
      "Epoch 7/25\n",
      "359/359 [==============================] - 157s 437ms/step - loss: 0.1292 - acc: 0.9483 - val_loss: 0.1463 - val_acc: 0.9483\n",
      "Epoch 8/25\n",
      "359/359 [==============================] - 153s 425ms/step - loss: 0.1278 - acc: 0.9497 - val_loss: 0.1438 - val_acc: 0.9489\n",
      "Epoch 9/25\n",
      "359/359 [==============================] - 157s 437ms/step - loss: 0.1233 - acc: 0.9516 - val_loss: 0.1447 - val_acc: 0.9437\n",
      "Epoch 10/25\n",
      "359/359 [==============================] - 154s 429ms/step - loss: 0.1178 - acc: 0.9538 - val_loss: 0.1362 - val_acc: 0.9530\n",
      "Epoch 11/25\n",
      "359/359 [==============================] - 156s 434ms/step - loss: 0.1068 - acc: 0.9566 - val_loss: 0.1390 - val_acc: 0.9483\n",
      "Epoch 12/25\n",
      "359/359 [==============================] - 152s 424ms/step - loss: 0.1044 - acc: 0.9583 - val_loss: 0.1370 - val_acc: 0.9468\n",
      "Epoch 13/25\n",
      "359/359 [==============================] - 159s 443ms/step - loss: 0.0975 - acc: 0.9621 - val_loss: 0.1482 - val_acc: 0.9509\n",
      "Epoch 14/25\n",
      "359/359 [==============================] - 155s 432ms/step - loss: 0.0901 - acc: 0.9641 - val_loss: 0.1776 - val_acc: 0.9509\n",
      "Epoch 15/25\n",
      "359/359 [==============================] - 149s 414ms/step - loss: 0.0914 - acc: 0.9652 - val_loss: 0.1435 - val_acc: 0.9556\n",
      "Epoch 16/25\n",
      "359/359 [==============================] - 162s 452ms/step - loss: 0.0814 - acc: 0.9672 - val_loss: 0.1455 - val_acc: 0.9504\n",
      "Epoch 17/25\n",
      "359/359 [==============================] - 152s 425ms/step - loss: 0.0827 - acc: 0.9678 - val_loss: 0.1273 - val_acc: 0.9613\n",
      "Epoch 18/25\n",
      "359/359 [==============================] - 160s 445ms/step - loss: 0.0774 - acc: 0.9698 - val_loss: 0.2125 - val_acc: 0.9442\n",
      "Epoch 19/25\n",
      "359/359 [==============================] - 152s 424ms/step - loss: 0.0771 - acc: 0.9700 - val_loss: 0.1441 - val_acc: 0.9545\n",
      "Epoch 20/25\n",
      "359/359 [==============================] - 156s 435ms/step - loss: 0.0728 - acc: 0.9710 - val_loss: 0.1518 - val_acc: 0.9602\n",
      "Epoch 21/25\n",
      "359/359 [==============================] - 161s 448ms/step - loss: 0.0667 - acc: 0.9736 - val_loss: 0.1819 - val_acc: 0.9592\n",
      "Epoch 22/25\n",
      "359/359 [==============================] - 152s 422ms/step - loss: 0.0657 - acc: 0.9733 - val_loss: 0.1918 - val_acc: 0.9499\n",
      "Epoch 23/25\n",
      "359/359 [==============================] - 149s 416ms/step - loss: 0.0644 - acc: 0.9750 - val_loss: 0.1408 - val_acc: 0.9669\n",
      "Epoch 24/25\n",
      "359/359 [==============================] - 156s 434ms/step - loss: 0.0591 - acc: 0.9759 - val_loss: 0.1690 - val_acc: 0.9499\n",
      "Epoch 25/25\n",
      "359/359 [==============================] - 152s 423ms/step - loss: 0.0596 - acc: 0.9768 - val_loss: 0.1681 - val_acc: 0.9561\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7af9e57da0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=num_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=num_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
